{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7de6062",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed13faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from requests_html import HTML\n",
    "from requests_html import HTMLSession\n",
    "from lxml import etree\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import xml.etree.ElementTree as ET\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8042c931",
   "metadata": {},
   "source": [
    "# Global Variables\n",
    "- `URL`: The URL we are scraping. This is an RSS feed URL, so we will be parsing XML\n",
    "- `NUM_OF_ARTICLES`: The number of articles the script will capture. `0` means all articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b00eadfd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "URL = 'https://cointelegraph.com/rss'\n",
    "NUM_OF_ARTICLES = 10\n",
    "GOOGLE_API_KEY_LOCATION = './keys/key.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab41f8a8",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb528f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source(url):\n",
    "    try:\n",
    "        session = HTMLSession()\n",
    "        response = session.get(url)\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error ->\", e)\n",
    "        return None\n",
    "\n",
    "def remove_html_tags(string):\n",
    "    soup = BeautifulSoup(string)\n",
    "    return soup.get_text()\n",
    "\n",
    "def get_article_content(soup):\n",
    "    \"\"\"returns article content, including html tags\"\"\"\n",
    "    article_content = soup.find('div', {'class':'post-content'})\n",
    "    if article_content != None:\n",
    "        return article_content\n",
    "    article_content = soup.find('div', {'class':'explained-post-content__item'})\n",
    "    if article_content != None:\n",
    "        return article_content\n",
    "    \n",
    "def get_image(image_url):\n",
    "    return f'=IMAGE(\"{image_url}\")'\n",
    "    \n",
    "def get_feed(url):\n",
    "    \"\"\"return a df of articles from url\"\"\"\n",
    "    response = get_source(url)\n",
    "    df = pd.DataFrame(columns = ['Title', 'Publication Date', 'Article URL', 'Description', 'Article Content'])\n",
    "    xml = etree.fromstring(response.content)\n",
    "    nth_article = 0\n",
    "    for item in xml.xpath('/rss/channel/item'):\n",
    "        nth_article = nth_article + 1\n",
    "        title = item.xpath('./title/text()')[0]\n",
    "        guid = item.xpath('./guid/text()')[0]\n",
    "        pubDate = item.xpath('./pubDate/text()')[0]\n",
    "        description = item.xpath('./description/text()')[0]\n",
    "        description = remove_html_tags(description)\n",
    "        image_url = item.xpath(\"./*[local-name()='content']/@url\")[0]\n",
    "        image = get_image(image_url)\n",
    "        creator = item.xpath(\"./*[local-name()='creator']/text()\")[0]\n",
    "        \n",
    "        # get html content using chrome webdriver:\n",
    "        driver.get(guid)\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        article_content = get_article_content(soup)\n",
    "        \n",
    "        row = {'Title': title, 'Publication Date': pubDate, 'Creator':creator, 'Article URL': guid, 'Description': description, 'image_url':image_url, 'image':image, 'Article Content':article_content}\n",
    "        df = df.append(row, ignore_index=True)\n",
    "        if nth_article == NUM_OF_ARTICLES:\n",
    "            break\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd4d723",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034f8899",
   "metadata": {},
   "source": [
    "## Iinitialize Webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e96c5cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 104.0.5112\n",
      "[WDM] - Get LATEST chromedriver version for 104.0.5112 google-chrome\n",
      "[WDM] - About to download new driver from https://chromedriver.storage.googleapis.com/104.0.5112.79/chromedriver_linux64.zip\n",
      "[WDM] - Driver has been saved in cache [/home/ff/.wdm/drivers/chromedriver/linux64/104.0.5112.79]\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "chrome_options = webdriver.ChromeOptions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0743d407",
   "metadata": {},
   "source": [
    "## Create/Update Spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bf40ac8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = get_feed(URL)\n",
    "df.to_csv('cointelegraph_rss_scrape_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff839b5",
   "metadata": {},
   "source": [
    "## Send CSV File to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0134b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "scope = [\"https://spreadsheets.google.com/feeds\", 'https://www.googleapis.com/auth/spreadsheets',\n",
    "         \"https://www.googleapis.com/auth/drive.file\", \"https://www.googleapis.com/auth/drive\"]\n",
    "\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name('./keys/key.json', scope)\n",
    "client = gspread.authorize(credentials)\n",
    "spreadsheet = client.open('web_scrape')\n",
    "\n",
    "with open('cointelegraph_rss_scrape_output.csv', 'r') as file_obj:\n",
    "    content = file_obj.read()\n",
    "    client.import_csv(spreadsheet.id, data=content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
